
# Integration Testing Workflow

This document describes a recommended workflow for designing and implementing **integration tests** in a codebase that favors testing functionality (rather than implementation details). It explains ***why*** we test the way we do and how we balance integration tests with occasional targeted unit tests.

---

## Overview

Testing properly with AI can be challenging—often more so than developing new features. While AI can easily generate tests, those tests are not always valuable in the long run. We have observed that **automated creation of unit tests without careful consideration** often produces brittle, tightly coupled tests that break upon even minor code changes, offering little real value. To avoid this pitfall, we focus on **Black Box Testing**: treating each feature as a whole, identifying its inputs and outputs, and ensuring its external behavior remains consistent

---

## Core Principles

1. **Test the Functionality, Not the Implementation**  
   - Write tests that consider the service or endpoint as a "black box."  
   - Provide known inputs and verify that the outputs, side effects, or responses match the expected results.

2. **Mock External Dependencies, Not Internal Modules**  
   - **Do mock**: Databases, external APIs, file systems, or any third-party service.  
   - **Don’t mock**: Classes, functions, or modules within your own codebase—test them together to ensure realistic coverage.  

3. **Prefer Integration Tests Over Unit Tests**  
   - Integration tests often uncover issues missed by unit tests because they simulate real-world usage and interactions between components.  
   - Use unit tests only where they add clear value (e.g., checking complex utility functions or pure algorithmic code that can fail independently).  

4. **Avoid Brittle Tests**  
   - Avoid hardcoding details of internal function calls, class structures, or other code-level specifics.  
   - If minor refactors break a large portion of your test suite, it means you may be testing implementation details rather than functional outcomes.

---

## Why Integration Tests?

Integration tests ensure your components work together correctly under realistic conditions. By focusing on ***external behavior*** (inputs and outputs) rather than ***internal implementation***, the tests can:
- Provide more robust coverage for real-world workflows.  
- Reduce test brittleness, allowing safe refactoring without breaking tests.  
- Document how the system should behave for end-users or dependent services.

---

## Why This Approach is Better than LLM-Generated Unit Tests

LLMs often produce highly coupled unit tests that mirror the implementation details unless expressly told otherwise—and even then, they may default to this pattern. Testing at this level can lead to:

- **Brittle Coverage**: When every internal detail is tested in isolation, even minor refactors break a significant portion of the tests.  
- **“Marking Its Own Homework”**: Having the LLM generate tests for the same functions it wrote may simply confirm the logic it already encoded, rather than assert correct outcomes for real use cases.  
- **Excessive Maintenance Overhead**: High coverage plus tightly coupled tests means any change triggers widespread test failures, slowing down development.  
- **Focus on Implementation Rather Than Results**: Unit tests often focus on the *how*, whereas integration tests focus on the *what*. With an LLM rapidly changing the *how*, testing the *what* (expected behavior) provides stability and confidence in real-world scenarios.

By emphasizing integration tests that treat your code as a black box and validate outcomes, you gain a more reliable, maintainable, and genuinely useful test suite—even if the underlying implementation is frequently regenerated by an LLM.

---

## High-Level Workflow

1. **User Specifies Feature and Location**  
   - Provide the feature or agent name and its file path in the codebase.  
   - This context ensures the LLM knows exactly what is being tested and where it resides.

2. **LLM Identifies Entry Points and Dependencies**  
   - The LLM scans the feature to discover all public-facing methods or endpoints.  
   - It notes external dependencies—such as databases, third-party services, or other modules that need to be mocked.

3. **LLM Creates a Test Plan**  
   - The LLM proposes integration test scenarios, input/output structures, and how to mock dependencies.  
   - It outlines both typical “happy path” scenarios and critical edge cases.  
   - The user reviews this plan, gives feedback, and clarifies any missing requirements.

4. **LLM Generates Integration Tests**  
   - The LLM writes the actual test files, following the agreed-upon scenarios.  
   - Tests are designed to validate the feature’s “black box” behavior, not internal implementation details.

5. **User Reviews and Merges**  
   - The user verifies correctness, maintainability, and alignment with project standards.  
   - Once approved, tests are merged into the main codebase.

---

## Testing Design Philosophy

- **Given-When-Then Format**  
  - Tests follow a clear, structured narrative:
    - **Given**: Setup or preconditions (test data, mock configurations).  
    - **When**: Execution of the core logic (calling the endpoint or function).  
    - **Then**: Validation that the outcome matches expected results.

- **Realistic Mocking of External Services**  
  - Only mock *truly external* systems like databases or third-party APIs.  
  - Simulate realistic responses and error conditions to ensure thorough testing.  
  - Avoid mocking internal modules or classes; test them together to capture real interactions.

- **Outcome-Focused**  
  - Emphasize verifying final responses, returned data, and side effects instead of internal method calls.  
  - Keep tests resilient to underlying code changes and refactors.

---

## Why This Approach Works

1. **Resilience to Change**  
   By focusing on outcomes (rather than internal function calls), tests allow for safe refactoring. As long as the “contract” remains intact, the test suite will still pass.

2. **Clarity & Maintainability**  
   Each test case tells a story about how a user or another system interacts with this feature. This is more intuitive than verifying a series of function calls or mocking internal methods.

3. **Meaningful Coverage**  
   Having experienced the downsides of chasing 100% coverage with brittle unit tests, we now seek *meaningful coverage*—covering realistic usage scenarios that reflect how the system is actually used.

4. **Reduced Duplicate Testing Effort**  
   Integration tests naturally cover multiple modules simultaneously, so we avoid writing separate tests for each function that do not add additional value.

---

## Balancing Integration and Unit Tests

While we emphasize integration tests, unit tests still have a place, such as:

- **Complex Pure Functions**  
  Functions with intricate logic or algorithms that can be tested in isolation, ensuring correctness of boundary conditions.
- **Utility Libraries**  
  Code that is loosely coupled to the main flow but used in multiple places can benefit from unit tests for quick coverage and confidence.
- **Performance Critical Sections**  
  When you need to ensure a small piece of the code is highly optimized, targeted testing might be necessary.

If your integration test already covers a specific logic path comprehensively, a separate unit test may be unnecessary—unless you need explicit clarity for debugging or further documentation.

---

## Conclusions & Take-Homes

- **AI Can Generate Tests, But Value Matters**: Generating code (including tests) with AI is fast, but without proper strategy and planning, the tests can be fragile and noisy.  
- **Focus on External Behavior**: Test the *what*, not the *how*, to keep your suite robust against internal changes.  
- **Mock External Dependencies Only**: Aim for realistic interactions and avoid mimicking internal classes or modules.  
- **Integration First, Unit When Needed**: Integration tests form a strong foundation; unit tests fill in gaps where specialized logic or performance concerns warrant focused coverage.

By following these guidelines, teams can create an effective, maintainable test suite that stays robust through AI-driven code evolutions and supports long-term stability.
