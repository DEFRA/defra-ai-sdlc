**Prompt add test coverage**

```
Please improve test coverage for the classifications API endpoints and the dependent modules listed below.  

## Current Coverage  
1. These are the coverage reports for modules have missing coverage
[PASTE TEST COVERAGE HERE, EXAMPLE: Name                                           Stmts   Miss  Cover   Missing
src/repositories/classification_repo.py           70     18    74%   44-56, 72, 77, 83, 97-99]

## Implemention steps
1. First check the actual routes in src/api/v1/classifications.py  
  
2. Review existing tests in tests/integration/test_classifications_api.py 

3. Add missing test scenarios ONLY for existing functionality:
- Focus on error cases and edge cases for the 3 existing routes  
- Do not add tests for non-existent routes/functionality  
  
1. Follow project testing standards from .cursor/rules/02_testing.mdc:  
- Given-When-Then pattern  
- Mock database operations appropriately  
- Use existing test utilities and fixtures  
  
1. Copy existing integration tests patterns and conventions in test_classifications_api.py
- Improve test coverage in but do so using integration tests that test API's and mock external dependencies 

Add tests only to improve coverage of existing code paths.
```

Follow up: 

```
this is the coverage report now:
src/repositories/classification_repo.py 70 18 74% 44-56, 72, 77, 83, 97-99  
  
Analyse @classification_repo.py - does it actually need the code for the lines that are missing - should we refactor?  
  
Or are we still missing some integreation tests in @test_classifications_api.py?
```


**prompt-fix-test-errors**

```
For this failing test/error/warning:
[failure]

Note, if we have removed the functionality from the codebase being tested, then remove the test as it's no longer needed.

1. Run this test file only - verify the test itself is correct
2. Think through mocking strategy and fix only this error
3. Provide complete implementation (no placeholders)
4. Don't change the source file, only fix the test file issues
5. Run tests to confirm/verify the fix
6. if the tests fail, after each change, stop, analyse, think step by step before you attempt to fix the issue

Reference other tests for common test patterns, such as how mocking works and async testing works.
```

```
# Test Analysis and Fix Requirements

- Failing Test: [paste coverage report line here]
- Context: Recent codebase refactoring has caused test failures

## Analysis Phase
1. Run isolated test and verify test implementation
2. Review target code functionality:
   - Examine code under test
   - Compare expected vs actual behavior
   - Identify any API or interface changes from refactoring
3. Determine root cause (test or code issue):
   - Document specific reasons for determination
   - Highlight any breaking changes identified

## Implementation Phase
### If Code Issue Detected:
1. Review existing test patterns and mocking approaches
2. Develop implementation plan:
   - Map required changes
   - Identify potential side effects
   - Consider edge cases
3. Provide complete, production-ready code implementation
4. Ensure backward compatibility where possible

### If Test Issue Detected:
1. Review existing test patterns and mocking approaches
2. Plan test updates:
   - Map required assertions
   - Consider edge cases
   - Maintain existing test style
3. Provide complete, production-ready test implementation
4. Ensure proper error messages and documentation

## Verification Loop
1. Run updated test in isolation
2. Address any linting/style warnings
3. Execute full test suite for affected module
4. If any issues found:
   - Document specific failures or warnings
   - Return to Analysis Phase
   - Repeat process until all checks pass
5. Final verification:
   - 100% coverage for modified files
   - All tests passing
   - No warnings or style issues
   - No regressions

## Output Requirements
- Provide single, complete implementation
- Include all necessary imports and setup
- Follow existing codebase conventions
- No partial solutions or iterations
- Silent reasoning process
- Document any iterations required in final implementation
```

**make-integration-test-strategy**

```
Write a test that calls the APIs, mocks mongo and tests the 3 module (endpoint, service, repo {db_handling})

aim is to test the functionality of the endpoint. If we changed the code but the functionality remained the same the tests shouldn't break!!

"test the functionality not the implementation".

Antipattern - test specific functions in each module which means slight code changes break everything! 

black box test

input --> {black box code} --> outputs
mock black box dependencies and test that a given input gives an expected output. Write tests to do that.

Can we design supplementary unit test to fill in the useful gaps left by the integration tests.


Workflow:
1. Scan the codebase to determine where intregation tests are needed
	1. Give it guided instructions. Eg. we want to test classifications, call the functions and mock the database. Scan the codebase. (do we want it to look at architecture docs to get context?)
	2. Call the API and test the response
2. Determine whether unit test are needed to fill in functionality gaps (but dont go for 100% coverage for no reason) - make a useful criteria for this!


**Get this info into the prompt for good concepts:**
test multiple units together in a functional way with external dependencies being mocked  
this will enable code refactoring without breaking tests i.e. preventing brittle tests  
tests should also document the functionality of the applicationguidelines:  
tests should be clear, easy to read and reason about  
test the functionality, not the implementation  
test multiple units together in a functional way  
mock external dependencies, but don't mock other classes or modules within the codebase itself  
prefer integration tests over unit tests whenever possibleimportant constraints:  
avoid brittle tests that will break if the code changes  
mock external dependencies, but don't mock other classes or modules within the codebase itself (edited)
```